"""
This file is responsible for parsing the tokens generated by the tokenizer.

Responsibilities include:
- Parsing the tokens into AST stacks
- Checking for syntax errors
- Checking for undeclared functions
- Warnings for undeclared variables
"""

# Custom modules
import command_manager
import error_handler
import app_static.parse.knip_tools as tools
import app_static.parse.knip_tokenizer as knip_tokenizer

class parser:
    """Parses tokens of KNIP files"""
    
    def __init__(self, project_id):
        """Initializes the parser."""
        
        self.project_id = project_id
        self.line = 1
        self.code = ""
        self.ast = [] # List of stacks
        
        # Token management
        self.tokens = []
        self.cur_token = 0
        
        # Call management - warn if not declared. not mandatory but recommended to declare them
        self.broadcasts = []
        self.lists = []
        self.dicts = []
        self.vars = []
        
        # Function 'orders'
        self.function_defs = {}
        
        self.error_handler = error_handler.error_handler(project_id)
        
    @tools.enforce_types
    def _checkForName(self, name: str):
        """
        Checks if the current token has the given name.
        
        ### Parameters:
        - name (str): The name to check for
        """
        
        # Perform check, and get current token for error message
        token = self.tokens[self.cur_token]       
        if not self.tokens[self.cur_token + 1]["name"] == name:
            self.error_handler.add_error(
                f"üî¢‚ùå - '{token}' expected the next token to be of type '{name}', but '{self.tokens[self.cur_token + 1]["name"]}' was provided",
                self.code[self.line-1], self.line
            )
            self.error_handler.throw_errors()
    
    @tools.enforce_types
    def _capture_tokens(self, startValue: str, endValue: str) -> list:
        """
        Captures tokens from self.tokens between tokens with name==startValue and name==endValue.
        """
        
        captured_tokens = []
        depth = 0  # track nested delimiters
        while self.cur_token + 1 < len(self.tokens):
            self.cur_token += 1
            arg_token = self.tokens[self.cur_token]
            
            if arg_token["name"] == startValue:
                depth += 1
                if depth != 1:
                    captured_tokens.append(arg_token)
                continue
            elif arg_token["name"] == endValue:
                depth -= 1
                if depth == 0:
                    break
                captured_tokens.append(arg_token)
                continue
            else:
                captured_tokens.append(arg_token)
        
        if not depth == 0:
            self.error_handler.add_error(
                f"üî¢‚ùå - Expected the next token to be of type '{endValue}', but reached the end of the file",
                self.code[self.line-1], self.line
            )
            self.error_handler.throw_errors()
                
        return captured_tokens
    
    @tools.enforce_types
    def _INTERNAL_process_token(self, tokens: list) -> list:
        """
        Processes the given tokens in isolation, returning a list of processed tokens.
        
        ### Parameters:
        - tokens (list): List of tokens to process
        
        ### Returns:
        - processed_tokens (list): List of processed tokens
        """
        
        processed_args = []
        # Save the current tokens and pointer
        temp_tokens = self.tokens
        temp_cur_token = self.cur_token
        
        # Switch to the provided token list
        self.tokens = tokens
        self.cur_token = -1
        
        while self.cur_token + 1 < len(tokens):
            self.cur_token += 1
            processed_token = self._process_token()
            if processed_token:
                processed_args.append(processed_token)
            
        # Restore previous state
        self.tokens = temp_tokens
        self.cur_token = temp_cur_token
        
        return processed_args
        
    def _process_token(self):
        """
        Processes the current token.
        """
        
        token = self.tokens[self.cur_token]
        
        match token["name"]:
            case "newline":
                self.line += 1
                return None
            case "function" | "reporter" | "hat":
                # Look up command info
                token_data = new_cmd_manager.get_command(token["value"])
                if not token_data:
                    self.error_handler.add_error(
                        f"Command not-found: '{token['value']}'", self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                
                opcode = token_data["opcode"]
                cmd_type = token_data["type"]
                args_list = token_data["args"]
                
                # Capture parenthesis arguments
                self._checkForName("lparen")
                arg_tokens = self._capture_tokens("lparen", "rparen")
                arg_tokens = self._INTERNAL_process_token(arg_tokens)
                all_args = dict(zip(args_list, arg_tokens))
                
                # Check for substacks (curly tokens) after the argument list
                if (self.cur_token + 1 < len(self.tokens) and self.tokens[self.cur_token + 1]["name"] == "lcurly"):
                    if "i.substack" in token_data['args']:
                        self._checkForName("lcurly")
                        substack_tokens = self._capture_tokens("lcurly", "rcurly")
                        substack_tokens = self._INTERNAL_process_token(substack_tokens)
                        all_args["substack"] = substack_tokens
                        
                        if "i.substack2" in token_data["args"]:
                            self._checkForName("lcurly")
                            substack2_tokens = self._capture_tokens("lcurly", "rcurly")
                            substack2_tokens = self._INTERNAL_process_token(substack2_tokens)
                            all_args["substack2"] = substack2_tokens
                    elif cmd_type == "hat": # Capture stack blocks, but hat blocks don't accept them as args, instead we will pass them as "stack"s
                        stack_tokens = self._capture_tokens("lcurly", "rcurly")
                        stack_tokens = self._INTERNAL_process_token(stack_tokens)
                    else:
                        self.error_handler.add_error(
                            f"üß©‚ÅâÔ∏è - '{token['value']}' received substack blocks, but does not expect any. "
                            f"Expected arguments: {token_data['args']}",
                            self.code[self.line-1], self.line
                        )
                        self.error_handler.throw_errors()
                
                if len(all_args) != len(args_list):
                    missing_arguments = [arg for arg in args_list if not all_args.get(arg)]
                    extraneous_arguments = [arg for arg in all_args if not arg in args_list]
                    print(missing_arguments, extraneous_arguments)
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Wrong number of arguments. '{token['value']}' expects "
                        f"[{len(args_list)}] arguments, but got [{len(all_args)}]",
                        self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                
                # Figure out return type. Hat blocks need their stacks returned in a seperate way
                if cmd_type == "hat":
                    return {"opcode": opcode, "type": cmd_type, "args": {}, "stack": stack_tokens}
                else:
                    return {"opcode": opcode, "type": cmd_type, "args": all_args}
            case "argument":
                return token
            case "functionDef":         
                # Capture the function definition
                func_name = token["value"]
                
                # Get kwargs and args
                self._checkForName("lparen")
                args = self._capture_tokens("lparen", "rparen")
                
                # Seperate and validate argument types
                kwargs = [arg for arg in args if arg["type"] == "kwarg"]
                miscArgs = [arg for arg in args if arg["type"] != "kwarg" and arg["type"] != "argumentDefinition"]
                if len(miscArgs) > 0:
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Invalid argument type. Expected 'argumentDefinition' or 'kwarg', but got '{miscArgs[0]['type']}'",
                        self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                args = [arg for arg in args if arg["type"] == "argumentDefinition"]
                
                # Get type
                self._checkForName("funcType")
                func_type = self.tokens[self.cur_token + 1]["type"]
                if not func_type in ["void", "num", "str", "bool"]:
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Invalid function type. Expected 'void', 'num', 'str', or 'bool', but got '{func_type}'",
                        self.code[self.line-1], self.line
                    )
                self.cur_token += 1
                
                # Get following stack
                self._checkForName("lcurly")
                stack_tokens = self._capture_tokens("lcurly", "rcurly")
                
                # Construct all the info for the block
                functionDef = {"opcode": "procedures_definition", "name": func_name, "type": "hat", "funcType": func_type, "args": args, "stack": stack_tokens}
                for kwarg in kwargs:
                    functionDef[kwarg["value"]["kwarg"]] = kwarg["value"]["value"]
                    
                # Validate or add function to function list
                self.function_defs[token["value"]] = (True, func_type)
                    
                # Add the function to the command manager
                return functionDef
            case "functionCall":
                # Add the function to function list
                if not self.function_defs.get(token["value"])[0]:
                    self.function_defs[token["value"]] = (False, "void")
                
                # Capture args for func call
                self._checkForName("lparen")
                arg_tokens = self._capture_tokens("lparen", "rparen")
                arg_tokens = self._INTERNAL_process_token(arg_tokens)
                return token
    
    def parse(self, tokens, code):
        """
        Parses the given KNP tokens.
        
        AST comprises of list of "stacks" containing blocks
        """
        
        self.tokens = tokens
        self.cur_token = -1
        self.code = tools._content_aware_split(code, "\n")
        self.line = 1
        
        self.broadcasts = []
        self.lists = []
        self.dicts = []
        self.vars = []
        
        while self.cur_token + 1 < len(tokens):
            self.cur_token += 1
            token = tokens[self.cur_token]
            
            # Handle newlines
            if token.get("name") == "newline":
                self.line += 1
                continue
            
            processed_token = self._process_token()
            if processed_token:
                self.ast.append(processed_token)
            
        return self.ast

with open("app_static\\generated_projects\\0\\log_0.txt", "w") as f:
    f.write("")
    
new_parser = parser(0)
new_tokenizer = knip_tokenizer.tokenizer(0)
new_cmd_manager = command_manager.command_manager(0)
with open("app_static\\scripts\\illegal.knp", "r") as f:
    code = f.read()
    ptokens = new_tokenizer.tokenize(code)
    print(new_parser.parse(ptokens, code))
    f.close()
