"""
This module is responsible for parsing the tokens generated by the tokenizer.
Responsibilities include:
- Parsing the tokens into AST stacks
- Checking for syntax errors
- Checking for undeclared functions
- Warnings for undeclared variables
"""

# Standard Library
import os
from typing import Optional, Dict, Any, List

# Third Party

# Local
import command_manager
import error_handler
import knip_tools as tools
import knip_tokenizer as knip_tokenizer
from custom_types import CommandArg, CommandInfo, Token

class Parser:
    """üõ†Ô∏è - Parses tokens of KNIP files"""
    
    def __init__(self, project_id: int):
        """Initializes the parser."""
        
        self.project_id = project_id
        self.line = 1
        self.code = ""
        self.ast = []  # List of AST stacks
        
        # Token management
        self.tokens: list[Token] = []
        self.current_token_index = 0
        
        # Call management: warning if functions/variables are not declared
        self.broadcasts = []
        self.lists = []
        self.dicts = []
        self.vars = []
        
        # Function definitions
        self.function_defs = {}
        
        self.error_handler = error_handler.Error_handler(project_id)
        
    @tools.enforce_types
    def _check_for_name(self, name: str):
        """
        Checks if the current token has the given name.
        
        Args:
          name (str): The name to check for
        """
        
        # Perform check and get current token for error message
        token = self.tokens[self.current_token_index]       
        if not self.tokens[self.current_token_index + 1]["name"] == name:
            self.error_handler.add_error(
                f"üî¢‚ùå - '{token}' expected the next token to be of type '{name}', but '{self.tokens[self.current_token_index + 1]['name']}' was provided",
                self.code[self.line-1], self.line
            )
            self.error_handler.throw_errors()
    
    @tools.enforce_types
    def _capture_tokens(self, start_value: str, end_value: str) -> list:
        """
        Captures tokens from self.tokens between tokens with name==start_value and name==end_value.
        
        Args:
          start_value (str): The name of the starting token
          end_value (str): The name of the ending token
        
        Returns:
          list: List of captured tokens
        """
        
        captured_tokens = []
        depth = 0  # Track nested delimiters
        while self.current_token_index + 1 < len(self.tokens):
            self.current_token_index += 1
            arg_token = self.tokens[self.current_token_index]
            
            if arg_token["name"] == start_value:
                depth += 1
                if depth != 1:
                    captured_tokens.append(arg_token)
                continue
            elif arg_token["name"] == end_value:
                depth -= 1
                if depth == 0:
                    break
                captured_tokens.append(arg_token)
                continue
            else:
                captured_tokens.append(arg_token)
        
        if depth != 0:
            self.error_handler.add_error(
                f"üî¢‚ùå - Expected the next token to be of type '{end_value}', but reached the end of the file",
                self.code[self.line-1], self.line
            )
            self.error_handler.throw_errors()
                
        return captured_tokens
    
    @tools.enforce_types
    def _internal_process_tokens(self, tokens: list) -> list:
        """
        Processes the given tokens in isolation, returning a list of processed tokens.
        
        Args:
          tokens (list): List of tokens to process
        
        Returns:
          list: List of processed tokens
        """
        
        processed_args = []
        # Save the current state
        temp_tokens = self.tokens
        temp_current_token_index = self.current_token_index
        
        # Switch to the provided token list
        self.tokens = tokens
        self.current_token_index = -1
        
        while self.current_token_index + 1 < len(tokens):
            self.current_token_index += 1
            processed_token = self._process_token()
            if processed_token:
                processed_args.append(processed_token)
            
        # Restore previous state
        self.tokens = temp_tokens
        self.current_token_index = temp_current_token_index
        
        return processed_args
        
    def _process_token(self):
        """
        Processes the current token and returns the processed result.
        
        Returns:
          dict: The processed token or None if not applicable
        """
        
        token = self.tokens[self.current_token_index]
        
        match token["name"]:
            case "newline":
                self.line += 1
                return None
            case "function" | "reporter" | "hat":
                # Look up command info
                token_data = new_cmd_manager.get_command(token["value"])
                if not token_data:
                    self.error_handler.add_error(
                        f"Command not found: '{token['value']}'", self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                
                opcode = token_data["opcode"]
                cmd_type = token_data["type"]
                args_list = token_data["args"]
                
                # Capture parenthesis arguments
                self._check_for_name("lparen")
                arg_tokens = self._capture_tokens("lparen", "rparen")
                arg_tokens = self._internal_process_tokens(arg_tokens)
                all_args = dict(zip(args_list, arg_tokens))
                
                # Check for substacks (curly tokens) after the argument list
                if (self.current_token_index + 1 < len(self.tokens) and self.tokens[self.current_token_index + 1]["name"] == "lcurly"):
                    if "i.substack" in token_data['args']:
                        self._check_for_name("lcurly")
                        substack_tokens = self._capture_tokens("lcurly", "rcurly")
                        substack_tokens = self._internal_process_tokens(substack_tokens)
                        all_args["substack"] = substack_tokens
                        
                        if "i.substack2" in token_data["args"]:
                            self._check_for_name("lcurly")
                            substack2_tokens = self._capture_tokens("lcurly", "rcurly")
                            substack2_tokens = self._internal_process_tokens(substack2_tokens)
                            all_args["substack2"] = substack2_tokens
                    elif cmd_type == "hat":  # Capture stack blocks; hat blocks don't accept them as args
                        stack_tokens = self._capture_tokens("lcurly", "rcurly")
                        stack_tokens = self._internal_process_tokens(stack_tokens)
                    else:
                        self.error_handler.add_error(
                            f"üß©‚ÅâÔ∏è - '{token['value']}' received substack blocks, but does not expect any. "
                            f"Expected arguments: {token_data['args']}",
                            self.code[self.line-1], self.line
                        )
                        self.error_handler.throw_errors()
                
                if len(all_args) != len(args_list):
                    missing_arguments = [arg for arg in args_list if not all_args.get(arg)]
                    extraneous_arguments = [arg for arg in all_args if arg not in args_list]
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Wrong number of arguments. '{token['value']}' expects "
                        f"[{len(args_list)}] arguments, but got [{len(all_args)}]",
                        self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                
                # Handle return type for hat blocks
                if cmd_type == "hat":
                    return {"opcode": opcode, "type": cmd_type, "args": {}, "stack": stack_tokens}
                else:
                    return {"opcode": opcode, "type": cmd_type, "args": all_args}
            case "argument":
                return token
            case "functionDef":
                # Capture the function definition
                func_name = token["value"]
                
                # Get kwargs and args
                self._check_for_name("lparen")
                args = self._capture_tokens("lparen", "rparen")
                
                # Separate and validate argument types
                kwargs = [arg for arg in args if arg["type"] == "kwarg"]
                misc_args = [arg for arg in args if arg["type"] != "kwarg" and arg["type"] != "argumentDefinition"]
                if len(misc_args) > 0:
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Invalid argument type. Expected 'argumentDefinition' or 'kwarg', but got '{misc_args[0]['type']}'",
                        self.code[self.line-1], self.line
                    )
                    self.error_handler.throw_errors()
                args = [arg for arg in args if arg["type"] == "argumentDefinition"]
                
                # Get return type
                self._check_for_name("funcType")
                func_type = self.tokens[self.current_token_index + 1]["type"]
                if func_type not in ["void", "num", "str", "bool"]:
                    self.error_handler.add_error(
                        f"üî¢‚ùå - Invalid function type. Expected 'void', 'num', 'str', or 'bool', but got '{func_type}'",
                        self.code[self.line-1], self.line
                    )
                self.current_token_index += 1
                
                # Get following stack
                self._check_for_name("lcurly")
                stack_tokens = self._capture_tokens("lcurly", "rcurly")
                
                # Construct all the info for the block
                function_def = {
                    "opcode": "procedures_definition",
                    "name": func_name,
                    "type": "hat",
                    "funcType": func_type,
                    "args": args,
                    "stack": stack_tokens
                }
                for kwarg in kwargs:
                    function_def[kwarg["value"]["kwarg"]] = kwarg["value"]["value"]
                    
                # Validate or add function to function list
                self.function_defs[token["value"]] = (True, func_type)
                    
                # Add the function to the command manager
                return function_def
            case "functionCall":
                # Add the function to function list
                if not self.function_defs.get(token["value"])[0]:
                    self.function_defs[token["value"]] = (False, "void")
                
                # Capture args for function call
                self._check_for_name("lparen")
                arg_tokens = self._capture_tokens("lparen", "rparen")
                arg_tokens = self._internal_process_tokens(arg_tokens)
                return token
    
    def parse(self, tokens, code):
        """
        Parses the given KNP tokens into an AST.
        
        The AST comprises a list of "stacks" containing blocks.
        
        Args:
          tokens (list): List of tokens to parse
          code (str): The source code corresponding to the tokens
        
        Returns:
          list: The generated AST
        """
        
        # Log the start of parsing
        self.error_handler.log("\nüõ†Ô∏è - Parsing tokens...")
        
        self.tokens = tokens
        self.current_token_index = -1
        self.code = tools._content_aware_split(code, "\n")
        self.line = 1
        
        self.broadcasts = []
        self.lists = []
        self.dicts = []
        self.vars = []
        
        while self.current_token_index + 1 < len(tokens):
            self.current_token_index += 1
            token = tokens[self.current_token_index]
            
            # Handle newlines
            if token.get("name") == "newline":
                self.line += 1
                continue
            
            processed_token = self._process_token()
            if processed_token:
                self.ast.append(processed_token)
            
        return self.ast

# Usage example
if __name__ == "__main__":
    with open("app_static\\generated_projects\\0\\log_0.txt", "w") as f:
        f.write("")
    
    new_parser = Parser(0)
    new_tokenizer = knip_tokenizer.Tokenizer(0, {})
    new_cmd_manager = command_manager.CommandManager(0)
    
    with open("app_static\\scripts\\illegal.knip", "r") as f:
        code = f.read()
        ptokens = new_tokenizer.tokenize(code)
        print(new_parser.parse(ptokens, code))
        f.close()